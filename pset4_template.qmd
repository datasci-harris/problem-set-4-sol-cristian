---
title: "Problem Set 4"
author: "Cristian Bancayan & Sol Rivas Lopes"
date: "2 November 2024"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points.

> “This submission is our work alone and complies with the 30538 integrity policy.” Add your initials to indicate your agreement: **CB** **SCRL**

> “I have uploaded the names of anyone else other than my partner and I worked with on the problem set here” (1 point)

> Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

### Question 1

```{python}
import warnings
import pandas as pd
import altair as alt
import datetime as dt
import numpy as np
import chardet
import os
import geopandas as gpd
import matplotlib.pyplot as plt

alt.renderers.enable("png")

warnings.filterwarnings('ignore')

# Working directory
# Cris
base_path = r"C:\Users\Cristian\Documents\GitHub\ppha30538_fall2024"
# Sol
# base_path = r"C:\Users\solch\OneDrive\Documentos\2024 - autumn quarter\python II"

```

After reading the tasks for this problem set, I pulled the PRVDR_CTGRY_CD, PRVDR_CTGRY_SBTYP_CD, PRVDR_NUM, PGM_TRMNTN_CD, FAC_NAME, ZIP_CD variables

### Question 2

#### Part (a)

```{python}

# reading in file for 2016
path = os.path.join(
    base_path, "problem-set-4-sol-cristian\pos2016.csv")

# Sol
#df_2016 = pd.read_csv(path)

# Cris needs to read in his files this way otherwise they won't open
# Sol can't open them unless it's with the syntax above
df_2016 = pd.read_csv(path, sep=';')

# Keeping both here and we will comment out as needed


# Subsetting to include only short-term hospitals
df_2016 = df_2016[(df_2016["PRVDR_CTGRY_CD"] == 1) & (df_2016["PRVDR_CTGRY_SBTYP_CD"] == 1)]


# Counting rows to get number of hospitals reported in data
row_count_2016 = df_2016.shape[0]

print(f"The number of hospitals reported in the data is {row_count_2016}")

```

#### Part (b)

> According to the Kaiser Family Foundation, "there are nearly 5,000 short-term, acute care hospitals in the United States." This number is considerably lower than the one we find in our data. There are two reasons that can explain this discrepancy. First, we know this datset will include hospitals that are no longer active, which would increase the number of facilities it is reporting. Second, this number references "acute care hospitals." These facilities \["primarily provides short-term medical treatment for patients with severe or urgent health issues. These institutions — common in small towns and rural areas — deliver comprehensive and specialized medical care for conditions that require immediate attention."\] {https://www.kff.org/report-section/a-look-at-rural-hospital-closures-and-implications-for-access-to-care-three-case-studies-issue-brief/}. Although our data is subsetted for short-term facilities, it can be considering other categories of hospitals, such as psychiatric, research, or specialty hospitals.

### Question 3

```{python}

#########################
###   2017 DATASET    ###
#########################

# reading in file for 2017
path = os.path.join(
    base_path, "problem-set-4-sol-cristian\pos2017.csv")

# Sol
#df_2017 = pd.read_csv(path)
# Cris
df_2017 = pd.read_csv(path, sep=';')

# Subsetting to include only short-term hospitals
df_2017 = df_2017[(df_2017["PRVDR_CTGRY_CD"] == 1) &
                  (df_2017["PRVDR_CTGRY_SBTYP_CD"] == 1)]


# Counting rows to get number of hospitals reported in data
row_count_2017 = df_2017.shape[0]

print(f"The number of hospitals reported in the 2017 data is {row_count_2017}")
```

```{python}

#########################
###   2018 DATASET    ###
#########################

# Setting path

path = os.path.join(
    base_path, "problem-set-4-sol-cristian\pos2018.csv")


# Sol: When trying to read in file, it seems that the encoding is different than what python assumes by default. I got an error, which I fed to ChatGPT
# It corrected my code to include the encoding:
# 
#df_2018 = pd.read_csv(path, encoding="ISO-8859-1")
# Cris worked fine
df_2018 = pd.read_csv(path, sep=';')

# Subsetting to include only short-term hospitals
df_2018 = df_2018[(df_2018["PRVDR_CTGRY_CD"] == 1) &
                  (df_2018["PRVDR_CTGRY_SBTYP_CD"] == 1)]


# Counting rows to get number of hospitals reported in data
row_count_2018 = df_2018.shape[0]

print(f"The number of hospitals reported in the 2018 data is {row_count_2018}")

```

```{python}

#########################
###   2019 DATASET    ###
#########################

# reading in file for 2019
path = os.path.join(
    base_path, "problem-set-4-sol-cristian\pos2019.csv")


# Sol
#df_2019 = pd.read_csv(path, encoding="ISO-8859-1")
# Cris
df_2019 = pd.read_csv(path, sep=';')


# Subsetting to include only short-term hospitals
df_2019 = df_2019[(df_2019["PRVDR_CTGRY_CD"] == 1) &
                  (df_2019["PRVDR_CTGRY_SBTYP_CD"] == 1)]


# Counting rows to get number of hospitals reported in data
row_count_2019 = df_2019.shape[0]

print(f"The number of hospitals reported in the 2019 data is {row_count_2019}")
```

```{python}

#########################
###     APPENDING     ###
#########################

# Adding a year column in each dataset to indentify it later on
df_2016["YEAR"] = 2016
df_2017["YEAR"] = 2017
df_2018["YEAR"] = 2018
df_2019["YEAR"] = 2019


# Appending datsets together
df_final = pd.concat([df_2016, df_2017, df_2018, df_2019],
                     ignore_index=True)


#########################
###     GRAPHING      ###
#########################


# Creating a small df with the number of observations each year
df_obs = {
    "Year": [2016, 2017, 2018, 2019],
    "Count": [row_count_2016, row_count_2017, row_count_2018, row_count_2019]
}
df_obs = pd.DataFrame(df_obs)


# Plotting
alt.Chart(df_obs, title="Number of Observations by Year").mark_bar().encode(
    alt.X("Year:N", title="Year"),
    alt.Y("Count:Q", title="Number of Observations")
).configure_axis(
    labelFontSize=8,
    titleFontSize=12
)
```

### Question 4

#### Part (a)

```{python}

# Getting unique provider numbers by year
df_unique = df_final.groupby("YEAR")["PRVDR_NUM"].nunique().reset_index()

# Plotting
alt.Chart(df_unique, title="Number of Unique Observations by Year").mark_bar().encode(
    alt.X("YEAR:N", title="Year"),
    alt.Y("PRVDR_NUM:Q", title="Number of Unique Observations")
).configure_axis(
    labelFontSize=8,
    titleFontSize=12
)

```

#### Part (b) FLAG

> looks the same? something might be wrong

## Identify hospital closures in POS file (15 pts) (\*)

### Question 1

```{python}

# Creating a data set with active hospitals in 2016
df_active2016 = df_2016[df_2016["PGM_TRMNTN_CD"] == 0]

# Creating a data set with observations from other years...
df_closed_by2019 = df_final[df_final["YEAR"]!=2016]
# ... and subsetting to only include non-active hospitals
df_closed_by2019 = df_closed_by2019[df_closed_by2019["PGM_TRMNTN_CD"]!=0]   

# Merging the two datasets
df_closed_by2019 = df_active2016.merge(df_closed_by2019[["PRVDR_NUM","PGM_TRMNTN_CD", "YEAR"]], on="PRVDR_NUM", how="left", indicator=True)

df_closed_by2019["_merge"].unique()
# Didn't find any active in 2016 that dissapear from the data later.

# Getting hospitals that were open in 2016 but closed in subsequent years
df_closed_by2019 = df_closed_by2019[df_closed_by2019["_merge"] == "both"]

# Sorting by provider number and then year to get all observations for same hospital together, in chronological order
df_closed_by2019.sort_values(by=["PRVDR_NUM", "YEAR_y"], ascending=[False, True], inplace=True)

# Keep the year in which the hospital has the status of not active for the first time.
df_closed_by2019 = df_closed_by2019.groupby("PRVDR_NUM").aggregate(
    FAC_NAME = ("FAC_NAME", "first"),
    ZIP_CD   = ("ZIP_CD", "first"),
    YEAR     = ("YEAR_y", "first")
).reset_index()

# Counting number of observartions
row_closed_by2019 = df_closed_by2019.shape[0]


print(f"Number of hospitals that were active in 2016 and were suspected to have closed by 2019: {row_closed_by2019}")
```

```{python}
# Creating a data set with active hospitals in 2016
df_active2016 = df_2016[df_2016["PGM_TRMNTN_CD"] == 0]

df_closed2017 = df_active2016.merge(df_2017[["PRVDR_NUM","PGM_TRMNTN_CD", "YEAR"]], on = "PRVDR_NUM", how = "left", indicator = True)
df_closed2017 = df_closed2017[df_closed2017["PGM_TRMNTN_CD_y"]!=0]

df_closed2018 = df_active2016.merge(df_2018[["PRVDR_NUM","PGM_TRMNTN_CD", "YEAR"]], on = "PRVDR_NUM", how = "left", indicator = True)
df_closed2018 = df_closed2018[df_closed2018["PGM_TRMNTN_CD_y"]!=0]

df_closed2019 = df_active2016.merge(df_2019[["PRVDR_NUM","PGM_TRMNTN_CD", "YEAR"]], on = "PRVDR_NUM", how = "left", indicator = True)
df_closed2019 = df_closed2019[df_closed2019["PGM_TRMNTN_CD_y"]!=0]

df_closedby2019_new = pd.concat([df_closed2017, df_closed2018,              df_closed2019], ignore_index=True)

df_closedby2019_new.sort_values(by=["PRVDR_NUM", "YEAR_y"], ascending=[False, True], inplace=True)
```
### Question 2

```{python}
# Sorting by name
df_closed_by2019.sort_values(by=['FAC_NAME'], inplace=True)
df_closed_by2019.head(10)
```

### Question 3

```{python}

# Creating a new dataset to work out of
df_active = df_final

# Creating a dummy column identifying active hospitals
df_active['ACTIVES'] = df_active['PGM_TRMNTN_CD'].apply(
    lambda x: 1 if x == 0 else 0)

# Creating a dummy column identifying non-active hospitals
df_active['NO_ACTIVE'] = df_active['PGM_TRMNTN_CD'].apply(
    lambda x: 1 if x != 0 else 0)

# Number of active, non active and total hospitals by zip and year
df_nactive_zip = df_active.groupby(["ZIP_CD","YEAR"]).aggregate(
    ACTIVE    = ("ACTIVES"  , "sum"),
    NO_ACTIVE = ("NO_ACTIVE", "sum"),
    TOTAL     = ("FAC_NAME" , "count")
).reset_index()

# Merge to identify in which ZIP CODE ocurre hospital closures
df_zip_fail = df_closed_by2019.merge(df_nactive_zip, on =["ZIP_CD"], how = "left", indicator=True)

df_zip_fail_new = df_zip_fail[(df_zip_fail['YEAR_x'] == df_zip_fail['YEAR_y']) | (df_zip_fail['YEAR_x'] - 1 == df_zip_fail['YEAR_y'])]

df_zip_fail = df_zip_fail.rename(columns={'_merge': 'merge_status'})df_zip_fail_new = df_zip_fail_new.rename(columns={'_merge': 'merge_status'})

comparison = df_zip_fail.merge(df_zip_fail_new, on="PRVDR_NUM", how="left", indicator = True)
comparison = comparison[comparison["_merge"]=="left_only"]

df_zip_fail['CHANGE_YEAR'] = df_active['PGM_TRMNTN_CD'].apply(
    lambda x: 1 if x != 0 else 0)

df_zip_fail['STATUS'] = 'Other'
df_zip_fail.loc[df_zip_fail['YEAR_x'] - 1 == df_zip_fail['YEAR_y'], 'STATUS'] = 'PAST'
df_zip_fail.loc[df_zip_fail['YEAR_x'] == df_zip_fail['YEAR_y'], 'STATUS'] = 'CHANGE'

df_zip_fail = df_zip_fail[["ZIP_CD","STATUS","ACTIVE"]]

df_zip_fail = df_zip_fail.pivot(index="ZIP_CD", columns="STATUS", values="ACTIVE").reset_index()

df_zip_fail["DECREASE"] = df_zip_fail["CHANGE"] - df_zip_fail["PAST"]

# Zip codes where the number of hospital do not decrease in the year of clousure
df_zip_fail = df_zip_fail[df_zip_fail["DECREASE"]>=0]
```

#### Part (a)

```{python}
df_closed_by2019 = df_closed_by2019.merge(df_zip_fail[["ZIP_CD","DECREASE"]], on = "ZIP_CD", how="left")

df_closed_by2019["DECREASE"].unique()

df_closed_by2019["POT_MERGE"] = df_closed_by2019["DECREASE"].apply(
    lambda x: 1 if x == 0 else 0)

pot_merge = df_closed_by2019[df_closed_by2019["POT_MERGE"]==1]["POT_MERGE"].value_counts().iloc[0]

print(f"Number of hospitals that fit in the definition of potentially being a merger/acquisition: {pot_merge}")

df_closed_by2019 = df_closed_by2019[df_closed_by2019["POT_MERGE"] != 1]
nhospital_corrected = df_closed_by2019.shape[0]

print(f"Number of hospitals left after correction for potential merge: {nhospital_corrected}")
```

#### Part (b)

```{python}
# Sorting by name
df_closed_by2019.sort_values(by=['FAC_NAME'], inplace=True)
df_closed_by2019.head(10)
```

## Download Census zip code shapefile (10 pt)

### Question 1

#### Part (a)

| File Extension | Description |
|----------------|------------------------------------------------------|
| **.dbf** | Stores attribute data in a table format, similar to a database or spreadsheet. |
| **.prj** | Contains projection and coordinate system information for accurate representation of the Earth's sphere into a plane. |
| **.shp** | Holds spatial data like points, lines, and polygons. |
| **.shx** | An index file linking the .shp spatial data to the attribute data in .dbf |
| **.xml** | Stores metadata |

All together, these files create the "shapefile," storing both attribute and spatial data.

#### Part (b)

```{python}


data_path = r"C:\Users\solch\OneDrive\Documentos\2024 - autumn quarter\python II\problem-set-4-sol-cristian\data"

# List of file paths
file_paths = ["gz_2010_us_860_00_500k.dbf ", "gz_2010_us_860_00_500k.prj ",
              "gz_2010_us_860_00_500k.shp", "gz_2010_us_860_00_500k.shx",
              "gz_2010_us_860_00_500k.xml"]

# Function to get file sizes


def get_file_sizes(paths):
    for file in paths:
        path = path = os.path.join(
            data_path, file)
        size = os.path.getsize(path)  # Size in bytes
        print(f"{os.path.basename(file)}: {size / 1024:.2f} KB")  # Size in KB
        # Got help from AI for conversion


# Measure and print file sizes
get_file_sizes(file_paths)

```

### Question 2

```{python}

# Reading in shapefile
path = path = os.path.join(data_path, "gz_2010_us_860_00_500k.shp")
shp_file = gpd.read_file(path)

# Getting column names to identify where I can find the ZIP
shp_file.columns

# Subsetting Texas
texas_shp = shp_file[shp_file["ZCTA5"].str.startswith(
    ("75", "76", "77", "78", "79"))]

# Creating new df to work on
df_2016_tx = df_2016
# Turning zip column into a string for better data handling
df_2016_tx["ZIP_CD"] = df_2016_tx["ZIP_CD"] .astype(str)
# Subsetting to only TX
df_2016_tx = df_2016_tx[df_2016_tx["ZIP_CD"].str.startswith(
    ("75", "76", "77", "78", "79")) & (df_2016_tx["ZIP_CD"].str.len() == 7)]
# Cleaning strings
df_2016_tx["ZIP_CD"] = df_2016_tx["ZIP_CD"].str.replace(
    '.0', '', regex=False)
# Counting hospitals by ZIP
df_2016_tx = df_2016_tx.groupby("ZIP_CD").size().reset_index()
# Renaming columns to merge properly
df_2016_tx.columns = ["ZCTA5", "COUNT"]
# Making NAs 0
df_2016_tx["COUNT"] = df_2016_tx["COUNT"].fillna(0)


# Merging
df_2016_tx = texas_shp.merge(df_2016_tx, on="ZCTA5", how="left")

fig, ax = plt.subplots(1, 1, figsize=(10, 8))
df_2016_tx.plot(column="COUNT", cmap='OrRd',
                    linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Number of Hospitals per Zip Code in Texas (2016)")
plt.show()



```



## Calculate zip code’s distance to the nearest hospital (20 pts) (\*)

### Question 1

### Question 2

### Question 3

### Question 4

#### Part (a)

#### Part (b)

#### Part (c)

#### Part (d)

### Question 5

#### Part (a)

#### Part (b)

#### Part (c)

## Effects of closures on access in Texas (15 pts)

### Question 1

### Question 2

### Question 3

### Question 4

## Reflecting on the exercise (10 pts)